{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3883792,"sourceType":"datasetVersion","datasetId":2307840},{"sourceId":6675836,"sourceType":"datasetVersion","datasetId":3851613}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.applications import ResNet50\nfrom keras.applications import InceptionV3\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nimport numpy as np\nfrom sklearn.metrics import classification_report\nimport time ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-26T23:39:45.348022Z","iopub.execute_input":"2023-11-26T23:39:45.348296Z","iopub.status.idle":"2023-11-26T23:40:03.210835Z","shell.execute_reply.started":"2023-11-26T23:39:45.348272Z","shell.execute_reply":"2023-11-26T23:40:03.209963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndirectory = '/kaggle/input/weedcrop-image-dataset/WeedCrop.v1i.yolov5pytorch/train'\ncontents = os.listdir(directory)\nnum_of_dirs = len([name for name in contents if os.path.isdir(os.path.join(directory, name))])\n\nprint(\"Contents of the directory:\")\nfor item in contents:\n    print(item)\n\nprint(f\"\\nNumber of directories: {num_of_dirs}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:40:03.212427Z","iopub.execute_input":"2023-11-26T23:40:03.212945Z","iopub.status.idle":"2023-11-26T23:40:03.228803Z","shell.execute_reply.started":"2023-11-26T23:40:03.212917Z","shell.execute_reply":"2023-11-26T23:40:03.227685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os\n\n# Define the directory path\ndirectory_path = '/kaggle/input/weedcrop-image-dataset/WewedCrop.v1i.yolov5pytorch/train'\n\n# List all files in the directory\nfile_names = os.listdir(directory_path)\n\n# Load images from the directory\nimages = []\nfor file_name in file_names:\n    if file_name.endswith('.png') or file_name.endswith('.jpg') or file_name.endswith('.jpeg'):\n        image_path = os.path.join(directory_path, file_name)\n        image = Image.open(image_path)\n        images.append(image)\n\n# Process the images as required\n# ...\n\n# Example: Showing the first image\nif images:\n    images[0].show()\nelse:\n    print(\"No images found in the directory.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:40:03.230426Z","iopub.execute_input":"2023-11-26T23:40:03.231102Z","iopub.status.idle":"2023-11-26T23:40:03.247918Z","shell.execute_reply.started":"2023-11-26T23:40:03.231075Z","shell.execute_reply":"2023-11-26T23:40:03.246879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define parameters\nbatch_size = 128\nnum_epochs = 10\nimage_size = (139, 139)\nnum_classes = 2\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:40:03.250668Z","iopub.execute_input":"2023-11-26T23:40:03.251006Z","iopub.status.idle":"2023-11-26T23:40:03.259504Z","shell.execute_reply.started":"2023-11-26T23:40:03.250974Z","shell.execute_reply":"2023-11-26T23:40:03.258816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the InceptionV3 model\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(*image_size, 3))\n\n# Freeze the layers of the base model\nfor layer in base_model.layers:\n    layer.trainable = False\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:40:03.260533Z","iopub.execute_input":"2023-11-26T23:40:03.261048Z","iopub.status.idle":"2023-11-26T23:40:11.876039Z","shell.execute_reply.started":"2023-11-26T23:40:03.261023Z","shell.execute_reply":"2023-11-26T23:40:11.875203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add custom classification layers\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(256, activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nclass_outputs = Dense(num_classes, activation='softmax')(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:40:11.877316Z","iopub.execute_input":"2023-11-26T23:40:11.877675Z","iopub.status.idle":"2023-11-26T23:40:11.947024Z","shell.execute_reply.started":"2023-11-26T23:40:11.877634Z","shell.execute_reply":"2023-11-26T23:40:11.946101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create the model\nmodel = Model(inputs=base_model.input, outputs=class_outputs)\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n\n# Load the training data with aggressive data augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=45,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_dataset = train_datagen.flow_from_directory(\n    '/kaggle/input/weedcrop-image-dataset/WeedCrop.v1i.yolov5pytorch/train',\n    target_size=(139, 139),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\n# Load the validation data with moderate data augmentation\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_dataset = val_datagen.flow_from_directory(\n    '/kaggle/input/weedcrop-image-dataset/WeedCrop.v1i.yolov5pytorch/valid',\n    target_size=(139, 139),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\n# Define learning rate scheduler\ndef lr_scheduler(epoch):\n    if epoch < 10:\n        return 0.001\n    elif 10 <= epoch < 20:\n        return 0.0001\n    else:\n        return 0.00001\n\nlr_schedule = LearningRateScheduler(lr_scheduler)\n\n# Define early stopping\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Define model checkpoint to save the best model\ncheckpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n# Define ReduceLROnPlateau callback\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    epochs=num_epochs,\n    validation_data=val_dataset,\n    callbacks=[lr_schedule, early_stop, checkpoint, reduce_lr]\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:40:11.948304Z","iopub.execute_input":"2023-11-26T23:40:11.949712Z","iopub.status.idle":"2023-11-26T23:45:16.856191Z","shell.execute_reply.started":"2023-11-26T23:40:11.949658Z","shell.execute_reply":"2023-11-26T23:45:16.855231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model in native Keras format\nmodel.save('plant_disease_model_inception.keras')\n\nimport joblib\n\n# Save the model using joblib\njoblib.dump(model, 'plant_disease_model_inception.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:16.857863Z","iopub.execute_input":"2023-11-26T23:45:16.858513Z","iopub.status.idle":"2023-11-26T23:45:19.469363Z","shell.execute_reply.started":"2023-11-26T23:45:16.858474Z","shell.execute_reply":"2023-11-26T23:45:19.468402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the metrics to visualize the training process\nimport matplotlib.pyplot as plt\n\ndef plot_metrics(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\nplot_metrics(history)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:19.471137Z","iopub.execute_input":"2023-11-26T23:45:19.471527Z","iopub.status.idle":"2023-11-26T23:45:20.050804Z","shell.execute_reply.started":"2023-11-26T23:45:19.471491Z","shell.execute_reply":"2023-11-26T23:45:20.049786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test data\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_dataset = test_datagen.flow_from_directory(\n    '/kaggle/input/weedcrop-image-dataset/WeedCrop.v1i.yolov5pytorch/test',\n    target_size=(139, 139),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_labels = test_dataset.classes\ntest_labels = to_categorical(test_labels, num_classes=num_classes)\n\nstart_time = time.time()\ny_pred = model.predict(test_dataset)\ny_pred_bool = np.argmax(y_pred, axis=1)\nrounded_labels = np.argmax(test_labels, axis=1)\n\nprint(classification_report(y_pred_bool, rounded_labels, digits=4))\nprint(\"Time taken to predict the model: \" + str(time.time() - start_time))\n\n# Save the model\nmodel.save('plant_disease_model_inception.h5')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:20.053563Z","iopub.execute_input":"2023-11-26T23:45:20.053907Z","iopub.status.idle":"2023-11-26T23:45:26.183737Z","shell.execute_reply.started":"2023-11-26T23:45:20.053880Z","shell.execute_reply":"2023-11-26T23:45:26.182892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n# Load your trained model\nmodel = tf.keras.models.load_model('/kaggle/working/plant_disease_model_inception.h5')  # Replace 'your_model_directory' with the path to your saved model\n\n# Load and preprocess your image\nimg_path = '/kaggle/input/weed-detection/test/ridderzuring_3126_jpg.rf.8980b3ae3ec4ecd023aab5bc54c26089.jpg'  # Replace 'path_to_your_image.jpg' with your image file path\nimg = image.load_img(img_path, target_size=(139, 139))  # Resize to match the input size of your model\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:26.185203Z","iopub.execute_input":"2023-11-26T23:45:26.185534Z","iopub.status.idle":"2023-11-26T23:45:29.264005Z","shell.execute_reply.started":"2023-11-26T23:45:26.185495Z","shell.execute_reply":"2023-11-26T23:45:29.263165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess your image\nimg_path = '/kaggle/input/weedcrop-image-dataset/WeedCrop.v1i.yolov5pytorch/test/images/12122_jpg.rf.d7313af9bab0e80b7149ee0a3c32caf8.jpg'  # Replace with the path to your image file\nimg = tf.keras.preprocessing.image.load_img(img_path, target_size=(139, 139))  # Load the image and resize\nimg_array = tf.keras.preprocessing.image.img_to_array(img)  # Convert image to array\nimg_array = tf.image.resize(img_array, (139, 139))  # Resize the image to match the model's input size\nimg_array = tf.expand_dims(img_array, axis=0)  # Add a batch dimension\n\n# Get the predictions for the image\npredictions = model.predict(img_array)\npredicted_class = tf.argmax(predictions[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:29.265294Z","iopub.execute_input":"2023-11-26T23:45:29.265688Z","iopub.status.idle":"2023-11-26T23:45:31.583333Z","shell.execute_reply.started":"2023-11-26T23:45:29.265650Z","shell.execute_reply":"2023-11-26T23:45:31.582571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predictions for the image\npredictions = model.predict(img_array)\npredicted_class = np.argmax(predictions[0])\n\n# Generate the heatmap\nlast_conv_layer = model.get_layer('mixed10')  \nheatmap_model = tf.keras.models.Model(model.inputs, [last_conv_layer.output, model.output])\n\nwith tf.GradientTape() as tape:\n    conv_outputs, predictions = heatmap_model(img_array)\n    loss = predictions[:, predicted_class]\n\ngrads = tape.gradient(loss, conv_outputs)\npooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\nheatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\nheatmap = np.maximum(heatmap, 0)\n\nheatmap_resized = cv2.resize(heatmap, (img_array.shape[2], img_array.shape[1]))\n\n# Convert both arrays to the same data type (e.g., unsigned 8-bit integer)\nimg_array_uint8 = (img_array[0].numpy() * 255).astype(np.uint8)\nheatmap_resized_uint8 = (heatmap_resized * 255).astype(np.uint8)  # Adjust the range of heatmap values\n\n# Overlay the heatmap on the original image\nheatmap_resized_uint8 = cv2.applyColorMap(heatmap_resized_uint8, cv2.COLORMAP_JET)\nsuperimposed_img = cv2.addWeighted(img_array_uint8, 0.6, heatmap_resized_uint8, 0.4, 0)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:31.584616Z","iopub.execute_input":"2023-11-26T23:45:31.585054Z","iopub.status.idle":"2023-11-26T23:45:34.801873Z","shell.execute_reply.started":"2023-11-26T23:45:31.585017Z","shell.execute_reply":"2023-11-26T23:45:34.800996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the original image, heatmap, and overlay\nplt.figure(figsize=(12, 6))\n\nplt.subplot(131)\nplt.imshow(img)\nplt.title('Original Image')\n\nplt.subplot(132)\nplt.imshow(heatmap_resized_uint8)\nplt.title('Heatmap')\n\nplt.subplot(133)\nplt.imshow(superimposed_img)\nplt.title('Overlay')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T23:45:34.803054Z","iopub.execute_input":"2023-11-26T23:45:34.803337Z","iopub.status.idle":"2023-11-26T23:45:35.937270Z","shell.execute_reply.started":"2023-11-26T23:45:34.803312Z","shell.execute_reply":"2023-11-26T23:45:35.936177Z"},"trusted":true},"execution_count":null,"outputs":[]}]}